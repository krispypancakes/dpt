{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a1938b36e945df8c5e81e6fc4240fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1630 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34c18ea0fcd40ad878f11d4742d0211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9672101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e930f2eb644a6a8d13d62c306cb162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fw_sample = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=\"sample-10BT\", cache_dir=\"data/raw/\", num_proc=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'id', 'dump', 'url', 'file_path', 'language', 'language_score', 'token_count', 'score', 'int_score'],\n",
       "        num_rows: 9672101\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf6a8bf06de4794a971786c34b2c008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=14):   0%|          | 0/9672101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "682847"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw_filtered = fw_sample[\"train\"].filter(lambda x: x[\"score\"] >= 3.69, num_proc=14)\n",
    "fw_filtered.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'id', 'dump', 'url', 'file_path', 'language', 'language_score', 'token_count', 'score', 'int_score'],\n",
       "    num_rows: 682847\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fw_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = fw_filtered.train_test_split(test_size=.1, seed=42)\n",
    "train_set = train_test[\"train\"]\n",
    "val_set = train_test[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82625492cd524edfa724eef353ac3536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/7 shards):   0%|          | 0/614562 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4496855de2824598852b366014c22c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/68285 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_set.save_to_disk(\"../data/train_fineweb_edu_369\")\n",
    "val_set.save_to_disk(\"../data/val_fineweb_edu_369\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['val_fineweb_edu_369', 'train_fineweb_edu_369']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state.json', 'dataset_info.json', 'data-00000-of-00001.arrow']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(\"../data/val_fineweb_edu_369\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_fineweb_edu_369\n",
      "state.json\n",
      "../data/val_fineweb_edu_369/state.json\n",
      "dataset_info.json\n",
      "../data/val_fineweb_edu_369/dataset_info.json\n",
      "data-00000-of-00001.arrow\n",
      "../data/val_fineweb_edu_369/data-00000-of-00001.arrow\n",
      "train_fineweb_edu_369\n",
      "state.json\n",
      "../data/train_fineweb_edu_369/state.json\n",
      "data-00005-of-00007.arrow\n",
      "../data/train_fineweb_edu_369/data-00005-of-00007.arrow\n",
      "dataset_info.json\n",
      "../data/train_fineweb_edu_369/dataset_info.json\n",
      "data-00000-of-00007.arrow\n",
      "../data/train_fineweb_edu_369/data-00000-of-00007.arrow\n",
      "data-00002-of-00007.arrow\n",
      "../data/train_fineweb_edu_369/data-00002-of-00007.arrow\n",
      "data-00004-of-00007.arrow\n",
      "../data/train_fineweb_edu_369/data-00004-of-00007.arrow\n",
      "data-00006-of-00007.arrow\n",
      "../data/train_fineweb_edu_369/data-00006-of-00007.arrow\n",
      "data-00003-of-00007.arrow\n",
      "../data/train_fineweb_edu_369/data-00003-of-00007.arrow\n",
      "data-00001-of-00007.arrow\n",
      "../data/train_fineweb_edu_369/data-00001-of-00007.arrow\n"
     ]
    }
   ],
   "source": [
    "base_path = \"../data\"\n",
    "for dir in os.listdir(base_path):\n",
    "    print(dir)\n",
    "    for _file in os.listdir(os.path.join(base_path, dir)):\n",
    "        print(_file)\n",
    "        print(os.path.join(base_path, dir, _file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/val_fineweb_edu_369'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(base_path, dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'val_fineweb_edu_369'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m os\u001b[38;5;241m.\u001b[39mlistdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_path, \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m)\u001b[49m))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'val_fineweb_edu_369'"
     ]
    }
   ],
   "source": [
    "os.listdir(os.path.join(base_path, os.listdir(dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_size = int(1e8) # shards fit 100mil tokens\n",
    "n_shards_train = 7\n",
    "n_shards_val = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "ds_train = load_from_disk(dataset_path=\"../data/train_fineweb_edu_369\")\n",
    "ds_val = load_from_disk(dataset_path=\"../data/val_fineweb_edu_369\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_single(text):\n",
    "    tokens = [tokenizer._special_tokens['<|endoftext|>']]\n",
    "    tokens.extend(tokenizer.encode_ordinary(text))\n",
    "    tokens_np = np.array(tokens)\n",
    "    return tokens_np.astype(np.uint16)\n",
    "\n",
    "def write_file(filename, np_tokens):\n",
    "    np.save(filename, np_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path: str) -> None:\n",
    "    ds = load_from_disk(dataset_path=path)\n",
    "    token_count = 0\n",
    "    shard_count = 0\n",
    "    np_tokens = np.empty((shard_size,), dtype=np.uint16)\n",
    "    for text in tqdm(ds[\"text\"]):\n",
    "        tokens = tokenize_single(text)\n",
    "        np_tokens[token_count:token_count+len(tokens)] = tokens\n",
    "        if token_count + len(tokens) < shard_size:\n",
    "            token_count += len(tokens)\n",
    "        else:\n",
    "            np_shard = os.path.join(path, f\"np_tokens_{shard_count}\")\n",
    "            write_file(np_shard, np_tokens)\n",
    "            shard_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (989,) into shape (4,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/train_fineweb_edu_369\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[56], line 8\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m tqdm(ds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m][:\u001b[38;5;241m5\u001b[39m]):\n\u001b[1;32m      7\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenize_single(text)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mnp_tokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken_count\u001b[49m\u001b[43m:\u001b[49m\u001b[43mtoken_count\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m tokens\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token_count \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[1;32m     10\u001b[0m         token_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokens)\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (989,) into shape (4,)"
     ]
    }
   ],
   "source": [
    "preprocess(\"../data/train_fineweb_edu_369\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663505152"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(ds[\"token_count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'id', 'dump', 'url', 'file_path', 'language', 'language_score', 'token_count', 'score', 'int_score'],\n",
       "    num_rows: 614562\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_train_short = np.load(\"../data/train_fineweb_edu_369/train_np_tokens_000001.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99999620,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_train_short.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.uint16(50256)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_train_short[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokens(filename):\n",
    "    npt = np.load(filename).astype(np.int32)\n",
    "    return torch.tensor(npt, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderFine:\n",
    "    def __init__(self, B, T, split, data_root) -> None:\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "        assert split in {\"train\", \"val\"}\n",
    "\n",
    "        # get the shard filenames\n",
    "        shards = os.listdir(data_root)\n",
    "        print(shards)\n",
    "        # we train on the numpy files\n",
    "        shards = [s for s in shards if s.split(\".\")[-1]==\".npy\"]\n",
    "        print(shards)\n",
    "        shards = sorted(shards)\n",
    "        self.shards = [os.path.join(data_root, s) for s in shards]\n",
    "        assert len(shards) > 0, f\"no shards found for split {split}\"\n",
    "        print(f\"found {len(shards)} shards for split {split}\")\n",
    "\n",
    "        self.current_shard = 0\n",
    "        self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "        self.current_position = self.B * self.T\n",
    "    \n",
    "    def next_batch(self) -> None:\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # outputs\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T\n",
    "        # if loading the next batch would be out of bounds, reset\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_shard = (self.current_shard + 1) % len(self.shards)\n",
    "            self.tokens = load_tokens(self.shards[self.current_shard])\n",
    "            self.current_position = B * T\n",
    "        return x, y     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['state.json', 'dataset_info.json', 'val_np_tokens_000000.npy', 'data-00000-of-00001.arrow']\n",
      "[]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "no shards found for split val",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoaderFine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/val_fineweb_edu_369\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[73], line 15\u001b[0m, in \u001b[0;36mDataLoaderFine.__init__\u001b[0;34m(self, B, T, split, data_root)\u001b[0m\n\u001b[1;32m     13\u001b[0m shards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(shards)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshards \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_root, s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m shards]\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shards) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno shards found for split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(shards)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m shards for split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_shard \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: no shards found for split val"
     ]
    }
   ],
   "source": [
    "val_loader = DataLoaderFine(B=10, T=1024, split=\"val\", data_root=\"../data/val_fineweb_edu_369\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['state.json', 'dataset_info.json', 'val_np_tokens_000000.npy', 'data-00000-of-00001.arrow']\n"
     ]
    }
   ],
   "source": [
    "data_root=\"../data/val_fineweb_edu_369\"\n",
    "\n",
    "shards_ = os.listdir(data_root)\n",
    "\n",
    "print(shards_)\n",
    "\n",
    "shards_ = [s for s in shards_ if s.split(\".\")[-1] == \"npy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
